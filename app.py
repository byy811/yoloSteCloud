"""
å¢å¼ºç‰ˆ YOLOv8 äº¤é€šæ ‡å¿—æ£€æµ‹ç³»ç»Ÿ
æ–°å¢åŠŸèƒ½: è§†é¢‘æ£€æµ‹ã€å®æ—¶æ‘„åƒå¤´ã€æ‰¹é‡å¤„ç†ã€æ¨¡å‹å¯¹æ¯”ã€æ€§èƒ½åˆ†æ
"""

import streamlit as st
from ultralytics import YOLO
from PIL import Image
import pandas as pd
import cv2
import numpy as np
import io
import tempfile
import time
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# é…ç½®
CLASS_NAMES = [
    'Green Light', 'Red Light',
    'Speed Limit 10', 'Speed Limit 100', 'Speed Limit 110', 'Speed Limit 120',
    'Speed Limit 20', 'Speed Limit 30', 'Speed Limit 40', 'Speed Limit 50',
    'Speed Limit 60', 'Speed Limit 70', 'Speed Limit 80', 'Speed Limit 90',
    'Stop'
]

# æ¨¡å‹è·¯å¾„é…ç½®
MODELS = {
    # 'åŸºçº¿æ¨¡å‹': 'runs/detect/yolov8n_baseline/weights/best.pt',
    # 'æ”¹è¿›æ¨¡å‹-æ•°æ®å¢å¼º': 'runs/detect/yolov8n_augment/weights/best.pt',
    # 'æ”¹è¿›æ¨¡å‹-CBAM': 'runs/detect/yolov8n_cbam/weights/best.pt',
    'æ”¹è¿›æ¨¡å‹-CBAM': 'runs/detect/yolov8n_cbam_method1/weights/best.onnx',
}

@st.cache_resource
def load_model(model_path):
    """åŠ è½½æ¨¡å‹å¹¶ç¼“å­˜"""
    try:
        model = YOLO(model_path)
        return model
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None


def detect_image(model, image, conf, iou, img_size):
    """å›¾åƒæ£€æµ‹"""
    results = model.predict(
        source=image,
        conf=conf,
        iou=iou,
        imgsz=img_size,
        save_conf=True,
        verbose=False
    )
    return results[0]


def detect_video(model, video_path, conf, iou, img_size, progress_bar):
    """è§†é¢‘æ£€æµ‹"""
    cap = cv2.VideoCapture(video_path)
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # è¾“å‡ºè§†é¢‘è®¾ç½®
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    output_path = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    frame_count = 0
    detection_stats = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # æ£€æµ‹
        results = model.predict(
            source=frame,
            conf=conf,
            iou=iou,
            imgsz=img_size,
            verbose=False
        )

        # ç»˜åˆ¶ç»“æœ
        annotated_frame = results[0].plot()
        out.write(annotated_frame)

        # ç»Ÿè®¡
        num_detections = len(results[0].boxes)
        detection_stats.append({
            'frame': frame_count,
            'detections': num_detections
        })

        frame_count += 1
        progress_bar.progress(frame_count / total_frames)

    cap.release()
    out.release()

    return output_path, pd.DataFrame(detection_stats)


def batch_process_images(model, uploaded_files, conf, iou, img_size):
    """æ‰¹é‡å¤„ç†å›¾ç‰‡"""
    results_list = []

    progress_bar = st.progress(0)
    status_text = st.empty()

    for idx, uploaded_file in enumerate(uploaded_files):
        status_text.text(f"å¤„ç†ä¸­: {uploaded_file.name} ({idx+1}/{len(uploaded_files)})")

        image = Image.open(uploaded_file)
        result = detect_image(model, image, conf, iou, img_size)

        # ç»Ÿè®¡ç»“æœ
        num_detections = len(result.boxes)
        detected_classes = [CLASS_NAMES[int(box.cls[0])] for box in result.boxes]

        results_list.append({
            'æ–‡ä»¶å': uploaded_file.name,
            'æ£€æµ‹æ•°é‡': num_detections,
            'æ£€æµ‹ç±»åˆ«': ', '.join(set(detected_classes)) if detected_classes else 'æ— '
        })

        progress_bar.progress((idx + 1) / len(uploaded_files))

    status_text.text("æ‰¹é‡å¤„ç†å®Œæˆ!")
    return pd.DataFrame(results_list)


def compare_models(image, models_dict, conf, iou, img_size):
    """å¯¹æ¯”å¤šä¸ªæ¨¡å‹"""
    comparison_results = []

    for model_name, model_path in models_dict.items():
        model = load_model(model_path)
        if model is None:
            continue

        start_time = time.time()
        result = detect_image(model, image, conf, iou, img_size)
        inference_time = time.time() - start_time

        comparison_results.append({
            'æ¨¡å‹': model_name,
            'æ£€æµ‹æ•°é‡': len(result.boxes),
            'æ¨ç†æ—¶é—´(s)': f"{inference_time:.3f}",
            'FPS': f"{1/inference_time:.1f}"
        })

        # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
        im_bgr = result.plot()
        im_rgb = cv2.cvtColor(im_bgr, cv2.COLOR_BGR2RGB)
        st.image(im_rgb, caption=model_name, use_container_width=True)

    return pd.DataFrame(comparison_results)


def plot_class_distribution(result):
    """ç»˜åˆ¶ç±»åˆ«åˆ†å¸ƒå›¾"""
    if len(result.boxes) == 0:
        return None

    class_counts = {}
    for box in result.boxes:
        cls_id = int(box.cls[0])
        cls_name = CLASS_NAMES[cls_id]
        class_counts[cls_name] = class_counts.get(cls_name, 0) + 1

    fig, ax = plt.subplots(figsize=(10, 6))
    classes = list(class_counts.keys())
    counts = list(class_counts.values())

    sns.barplot(x=counts, y=classes, palette='viridis', ax=ax)
    ax.set_xlabel('æ£€æµ‹æ•°é‡', fontsize=12)
    ax.set_ylabel('ç±»åˆ«', fontsize=12)
    ax.set_title('æ£€æµ‹ç±»åˆ«åˆ†å¸ƒ', fontsize=14, fontweight='bold')

    return fig


def main():
    st.set_page_config(
        page_title="YOLOv8 äº¤é€šæ ‡å¿—æ£€æµ‹ç³»ç»Ÿ",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title("ğŸš¦ æ”¹è¿›YOLOv8äº¤é€šæ ‡å¿—æ™ºèƒ½æ£€æµ‹ç³»ç»Ÿ")
    st.caption("æ¯•ä¸šè®¾è®¡é¡¹ç›® | æ”¯æŒå›¾ç‰‡/è§†é¢‘/å®æ—¶æ£€æµ‹ | å¤šæ¨¡å‹å¯¹æ¯”")
    st.markdown("---")

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")

        # åŠŸèƒ½é€‰æ‹©
        mode = st.selectbox(
            "é€‰æ‹©åŠŸèƒ½æ¨¡å¼",
            ["ğŸ“· å•å¼ å›¾ç‰‡æ£€æµ‹", "ğŸ¥ è§†é¢‘æ£€æµ‹", "ğŸ“¦ æ‰¹é‡å›¾ç‰‡å¤„ç†", "ğŸ” æ¨¡å‹æ€§èƒ½å¯¹æ¯”"]
        )

        st.markdown("---")

        # æ¨¡å‹é€‰æ‹©
        if mode != "ğŸ” æ¨¡å‹æ€§èƒ½å¯¹æ¯”":
            selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", list(MODELS.keys()))
            model_path = MODELS[selected_model]
            st.info(f"å½“å‰æ¨¡å‹: `{selected_model}`")

        st.markdown("---")

        # æ£€æµ‹å‚æ•°
        st.subheader("æ£€æµ‹å‚æ•°")
        conf = st.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.0, 1.0, 0.25, 0.05)
        iou = st.slider("IoUé˜ˆå€¼", 0.0, 1.0, 0.45, 0.05)
        img_size = st.selectbox("æ¨ç†å°ºå¯¸", [320, 640, 1280], index=1)

        st.markdown("---")
        st.caption("ğŸ’¡ æç¤º: è°ƒæ•´å‚æ•°å¯ä¼˜åŒ–æ£€æµ‹æ•ˆæœ")

    # ========== å•å¼ å›¾ç‰‡æ£€æµ‹ ==========
    if mode == "ğŸ“· å•å¼ å›¾ç‰‡æ£€æµ‹":
        model = load_model(model_path)
        if model is None:
            st.stop()

        uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡", type=["jpg", "jpeg", "png"])

        if uploaded_file:
            col1, col2 = st.columns(2)

            image = Image.open(uploaded_file)

            with col1:
                st.subheader("åŸå§‹å›¾ç‰‡")
                st.image(image, use_container_width=True)

            if st.button("ğŸš€ å¼€å§‹æ£€æµ‹", use_container_width=True):
                with st.spinner("æ£€æµ‹ä¸­..."):
                    result = detect_image(model, image, conf, iou, img_size)

                with col2:
                    st.subheader("æ£€æµ‹ç»“æœ")
                    im_bgr = result.plot()
                    im_rgb = cv2.cvtColor(im_bgr, cv2.COLOR_BGR2RGB)
                    st.image(im_rgb, use_container_width=True)

                # æ£€æµ‹è¯¦æƒ…
                if len(result.boxes) > 0:
                    st.subheader("ğŸ“Š æ£€æµ‹è¯¦æƒ…")

                    tab1, tab2 = st.tabs(["æ£€æµ‹åˆ—è¡¨", "ç±»åˆ«åˆ†å¸ƒ"])

                    with tab1:
                        rows = []
                        for i, box in enumerate(result.boxes):
                            cls_id = int(box.cls[0])
                            conf_val = float(box.conf[0])
                            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())

                            rows.append({
                                "åºå·": i + 1,
                                "ç±»åˆ«": CLASS_NAMES[cls_id],
                                "ç½®ä¿¡åº¦": f"{conf_val:.2%}",
                                "åæ ‡": f"({x1},{y1})-({x2},{y2})"
                            })

                        st.dataframe(pd.DataFrame(rows), use_container_width=True)

                    with tab2:
                        fig = plot_class_distribution(result)
                        if fig:
                            st.pyplot(fig)
                else:
                    st.info("âš  æœªæ£€æµ‹åˆ°äº¤é€šæ ‡å¿—")

    # ========== è§†é¢‘æ£€æµ‹ ==========
    elif mode == "ğŸ¥ è§†é¢‘æ£€æµ‹":
        model = load_model(model_path)
        if model is None:
            st.stop()

        uploaded_video = st.file_uploader("ä¸Šä¼ è§†é¢‘", type=["mp4", "avi", "mov"])

        if uploaded_video:
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            tfile = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
            tfile.write(uploaded_video.read())

            st.video(uploaded_video)

            if st.button("ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘"):
                progress_bar = st.progress(0)

                with st.spinner("è§†é¢‘å¤„ç†ä¸­,è¯·è€å¿ƒç­‰å¾…..."):
                    output_path, stats_df = detect_video(
                        model, tfile.name, conf, iou, img_size, progress_bar
                    )

                st.success("âœ… è§†é¢‘å¤„ç†å®Œæˆ!")

                # æ˜¾ç¤ºå¤„ç†åçš„è§†é¢‘
                st.video(output_path)

                # ç»Ÿè®¡ä¿¡æ¯
                st.subheader("ğŸ“ˆ æ£€æµ‹ç»Ÿè®¡")
                col1, col2, col3 = st.columns(3)
                col1.metric("æ€»å¸§æ•°", len(stats_df))
                col2.metric("å¹³å‡æ£€æµ‹æ•°", f"{stats_df['detections'].mean():.1f}")
                col3.metric("æœ€å¤§æ£€æµ‹æ•°", stats_df['detections'].max())

    # ========== æ‰¹é‡å¤„ç† ==========
    elif mode == "ğŸ“¦ æ‰¹é‡å›¾ç‰‡å¤„ç†":
        model = load_model(model_path)
        if model is None:
            st.stop()

        uploaded_files = st.file_uploader(
            "ä¸Šä¼ å¤šå¼ å›¾ç‰‡",
            type=["jpg", "jpeg", "png"],
            accept_multiple_files=True
        )

        if uploaded_files:
            st.info(f"å·²ä¸Šä¼  {len(uploaded_files)} å¼ å›¾ç‰‡")

            if st.button("ğŸ“¦ æ‰¹é‡å¤„ç†"):
                results_df = batch_process_images(
                    model, uploaded_files, conf, iou, img_size
                )

                st.subheader("å¤„ç†ç»“æœæ±‡æ€»")
                st.dataframe(results_df, use_container_width=True)

                # ç»Ÿè®¡
                col1, col2 = st.columns(2)
                col1.metric("å¤„ç†å›¾ç‰‡æ•°", len(results_df))
                col2.metric("æ€»æ£€æµ‹æ•°", results_df['æ£€æµ‹æ•°é‡'].sum())

    # ========== æ¨¡å‹å¯¹æ¯” ==========
    elif mode == "ğŸ” æ¨¡å‹æ€§èƒ½å¯¹æ¯”":
        uploaded_file = st.file_uploader("ä¸Šä¼ æµ‹è¯•å›¾ç‰‡", type=["jpg", "jpeg", "png"])

        if uploaded_file:
            image = Image.open(uploaded_file)

            st.subheader("åŸå§‹å›¾ç‰‡")
            st.image(image, use_container_width=True)

            if st.button("ğŸ” å¯¹æ¯”æ‰€æœ‰æ¨¡å‹"):
                st.subheader("æ¨¡å‹æ£€æµ‹ç»“æœå¯¹æ¯”")
                comparison_df = compare_models(image, MODELS, conf, iou, img_size)

                st.subheader("ğŸ“Š æ€§èƒ½å¯¹æ¯”è¡¨")
                st.dataframe(comparison_df, use_container_width=True)


if __name__ == "__main__":
    main()